# Experiment Suggestion Templates

Pre-built hypothesis templates for common Shopify app marketing experiments. Each template includes default ICE scores, a structured hypothesis, success criteria, and a trigger condition for when to run it.

---

## 1. Title A/B Test

- **Channel:** ASO / App Store
- **Funnel stage:** Discovery (impressions → listing views)
- **ICE:** Impact 7 / Confidence 6 / Ease 9 (ICE: 7.3)
- **Rationale:** Title is the #1 discovery driver. High ease because it's a single field change.

**Hypothesis:**
> If we change the app title from "[current title]" to "[new title with target keyword]", then listing views will increase by 15-25%, because 70%+ of installs come from in-store search and title keyword match is the strongest ranking signal.

**Success criteria:**
- Primary metric: Listing views per week
- Minimum effect: 15% increase
- Sample period: 2 weeks (minimum 200 impressions)

**Damage control:** Install conversion rate stays above current baseline (title change shouldn't hurt conversion).

**When to run:** Current title doesn't contain your highest-volume target keyword, or you've identified a higher-volume keyword variant.

---

## 2. Description Copy Change

- **Channel:** ASO / App Store
- **Funnel stage:** Conversion (listing views → "Add app" clicks)
- **ICE:** Impact 6 / Confidence 5 / Ease 8 (ICE: 6.3)
- **Rationale:** Description influences conversion but is read less than title/screenshots. Moderate confidence because copy impact varies.

**Hypothesis:**
> If we rewrite the first paragraph of the description to lead with [specific merchant pain point], then the "Add app" click rate will increase by 10-15%, because merchants scan the first 2-3 lines before deciding to read more.

**Success criteria:**
- Primary metric: "Add app" click rate (clicks / listing views)
- Minimum effect: 10% increase
- Sample period: 2 weeks (minimum 100 listing views)

**Damage control:** Listing views don't drop (description changes shouldn't affect search ranking).

**When to run:** "Add app" click rate is below 30% (benchmark: 20% average, 40%+ excellent) or you've recently updated your ICP and the listing copy doesn't match.

---

## 3. Screenshot Reorder/Redesign

- **Channel:** ASO / App Store
- **Funnel stage:** Conversion (listing views → "Add app" clicks)
- **ICE:** Impact 7 / Confidence 5 / Ease 7 (ICE: 6.3)
- **Rationale:** First screenshot is the most viewed element after the icon. Higher impact than description but lower ease (requires design work).

**Hypothesis:**
> If we change the first screenshot to show [specific value proposition / key feature], then the "Add app" click rate will increase by 15-20%, because the first screenshot is the primary visual selling point and merchants decide within 3 seconds.

**Success criteria:**
- Primary metric: "Add app" click rate
- Minimum effect: 15% increase
- Sample period: 2 weeks (minimum 100 listing views)

**Damage control:** Listing views stay stable (screenshot changes don't affect search ranking).

**When to run:** "Add app" click rate is below 30%, or first screenshot shows a generic dashboard instead of the specific value moment.

---

## 4. Keyword Addition

- **Channel:** ASO / App Store
- **Funnel stage:** Discovery (search impressions)
- **ICE:** Impact 6 / Confidence 7 / Ease 9 (ICE: 7.3)
- **Rationale:** Adding relevant keywords to title/description is high-confidence (known ranking signal) and very easy to execute.

**Hypothesis:**
> If we add "[target keyword]" to the app [title/description/subtitle], then search impressions for that keyword will increase by 20-50%, because Shopify App Store search heavily weights keyword presence in listing fields.

**Success criteria:**
- Primary metric: Search impressions for target keyword
- Minimum effect: 20% increase in impressions
- Sample period: 2 weeks

**Damage control:** Existing keyword rankings don't drop (check top 3 current keywords maintain position).

**When to run:** Keyword research reveals a relevant high-volume keyword not present in your listing, or a competitor ranks for a keyword you should own.

---

## 5. Outreach Template Variant

- **Channel:** Cold outreach
- **Funnel stage:** Reply rate (emails sent → replies)
- **ICE:** Impact 5 / Confidence 5 / Ease 9 (ICE: 6.3)
- **Rationale:** Easy to test (just change the email copy) but impact is limited to the outreach channel. Moderate confidence — email performance is unpredictable.

**Hypothesis:**
> If we change the Email 1 subject line from "[current subject]" to "[new subject with personalization]", then reply rate will increase by 5-10 percentage points, because [personalized/curiosity-driven/problem-first] subject lines outperform generic ones in B2B cold email.

**Success criteria:**
- Primary metric: Reply rate (replies / emails sent)
- Minimum effect: 5 percentage point increase
- Sample size: 30 emails per variant (minimum)

**Damage control:** Unsubscribe/complaint rate stays below 2%.

**When to run:** Current reply rate is below 10%, or you've been using the same template for 50+ sends without testing alternatives.

---

## 6. Pricing Experiment

- **Channel:** Monetization
- **Funnel stage:** Trial-to-paid conversion
- **ICE:** Impact 8 / Confidence 4 / Ease 5 (ICE: 5.7)
- **Rationale:** Pricing directly affects revenue per user (high impact) but is genuinely uncertain (low confidence) and harder to test cleanly on Shopify (moderate ease). This is a classic "big swing."

**Hypothesis:**
> If we [add a lower entry tier / increase trial length from X to Y days / add a usage-based component], then trial-to-paid conversion will increase by [X%], because [reasoning: lower barrier, more time to see value, aligned pricing with usage].

**Success criteria:**
- Primary metric: Trial-to-paid conversion rate
- Minimum effect: 20% relative improvement (e.g., 10% → 12%)
- Sample period: 4 weeks (minimum 50 trial starts)

**Damage control:** Monthly revenue doesn't drop more than 10% (lower pricing could increase conversion but reduce ARPU).

**When to run:** Trial-to-paid rate is below 10% (benchmark average), or you're seeing high trial adoption but low conversion, suggesting price/value mismatch.

---

## 7. Onboarding Flow Change

- **Channel:** Product / Retention
- **Funnel stage:** Activation (install → first value moment)
- **ICE:** Impact 8 / Confidence 4 / Ease 4 (ICE: 5.3)
- **Rationale:** Onboarding dramatically affects retention (high impact) but requires product changes (low ease) and the right change is hard to predict (low confidence). Another "big swing."

**Hypothesis:**
> If we [reduce onboarding from X steps to Y / add a guided setup wizard / change the first screen to show value before asking for configuration], then activation rate will increase by [X%], because 77% of daily active users are lost within 3 days and faster time-to-value reduces early abandonment.

**Success criteria:**
- Primary metric: Activation rate (installs completing key first action)
- Minimum effect: 15% relative improvement
- Sample period: 3-4 weeks (minimum 30 new installs)

**Damage control:** Uninstall rate within 7 days doesn't increase (simpler onboarding shouldn't cause more removals).

**When to run:** Install-to-activation rate is below 40% (benchmark: 40-60% average), or you see a significant drop-off at a specific onboarding step.

---

## 8. Review Ask Timing

- **Channel:** Social proof
- **Funnel stage:** Review velocity (active users → reviews)
- **ICE:** Impact 6 / Confidence 4 / Ease 7 (ICE: 5.7)
- **Rationale:** Review count drives organic discovery (need 20-60 for meaningful impact). Moderate ease (requires in-app prompt change). Low confidence because optimal timing varies by app.

**Hypothesis:**
> If we move the review request prompt from [current trigger] to [new trigger, e.g., after merchant's 3rd successful use of key feature], then review submission rate will increase by [X%], because merchants are most likely to leave a positive review immediately after experiencing value.

**Success criteria:**
- Primary metric: Review submission rate (prompts shown → reviews submitted)
- Minimum effect: 50% relative improvement (e.g., 2% → 3%)
- Sample period: 4 weeks (minimum 100 prompts shown)

**Damage control:** Average review rating doesn't drop below 4.5 stars (bad timing could trigger negative reviews).

**When to run:** Review velocity is below 1 review per 50 active users per month, or your current review prompt has a submission rate below 3%.
